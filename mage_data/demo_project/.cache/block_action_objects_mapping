{"block_file": {"custom/dataclass.py:custom:python:dataclass": {"content": "\nfrom demo_project.custom.sparkconnection import spark\nfrom pyspark.sql import types\nfrom pyspark.sql.functions import split\n\n@custom\nclass datafiles:\n    unique_artists_df_par = spark.read.parquet(\"datafiles/unique_artists\")\n    tracks_per_year_df_par = spark.read.parquet(\"datafiles/tracks_per_year\")\n    unique_tracks_df_par = spark.read.parquet(\"datafiles/unique_tracks\")\n\n", "file_path": "custom/dataclass.py", "language": "python", "type": "custom", "uuid": "dataclass"}, "custom/long_tree.py:custom:python:long tree": {"content": "import dlt\nfrom dlt.sources.helpers import requests\n\n# Specify the URL of the API endpoint\nurl = \"https://api.github.com/repos/dlt-hub/dlt/issues\"\n# Make a request and check if it was successful\nresponse = requests.get(url)\nresponse.raise_for_status()\n\npipeline = dlt.pipeline(\n    pipeline_name='github_issues',\n    destination='duckdb',\n    dataset_name='github_data',\n)\n# The response contains a list of issues\nload_info = pipeline.run(\n    response.json(),\n    table_name=\"issues\",\n    write_disposition=\"replace\"  # <-- Add this line\n)\n\nprint(load_info)", "file_path": "custom/long_tree.py", "language": "python", "type": "custom", "uuid": "long_tree"}, "custom/sparkconnection.py:custom:python:sparkconnection": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.master(os.getenv('SPARK_MASTER_HOST', 'local')).getOrCreate()\n\n@custom\ndef transform_custom(*args, **kwargs):\n\n    return spark.sql('select 1')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/sparkconnection.py", "language": "python", "type": "custom", "uuid": "sparkconnection"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/rough_paper.py:data_exporter:python:rough paper": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'your_bucket_name'\n    object_key = 'your_object_key'\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\n", "file_path": "data_exporters/rough_paper.py", "language": "python", "type": "data_exporter", "uuid": "rough_paper"}, "data_exporters/summer_wave.py:data_exporter:python:summer wave": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\nimport gspread\nfrom pydrive.auth import GoogleAuth\nfrom pydrive.drive import GoogleDrive\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(*args, **kwargs) -> None:\n\n    gauth = GoogleAuth()\n    drive = GoogleDrive(gauth)\n\n    file = drive.CreateFile({'title': 'metadata.yaml'})\n    file.SetContentString('Hello World!')  # or file.SetContentFile('path/to/my/file.txt')\n    file.Upload()\n\n    return print('Uploaded file with ID {}'.format(file.get('id')))\n", "file_path": "data_exporters/summer_wave.py", "language": "python", "type": "data_exporter", "uuid": "summer_wave"}, "data_loaders/broken_river.py:data_loader:python:broken river": {"content": "import pandas as pd\nimport requests\nimport numpy as np\nimport os\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\ndef get_gender(name):\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"format\": \"json\",\n        \"language\": \"en\",\n        \"search\": name\n    }\n    try:\n        data = requests.get(url, params=params)\n        gender_id = data.json()[\"search\"][0][\"id\"]\n        gender_url = f'https://www.wikidata.org/wiki/Special:EntityData/{gender_id}.json'\n        response = requests.get(gender_url)\n        gender_data = response.json()\n        try:\n            gender = gender_data['entities'][gender_id]['claims']['P21'][0]['mainsnak']['datavalue']['value']['id']\n            return gender\n        except KeyError:\n            return \"Error: Invalid JSON structure or key path not found.\"\n    except:\n        return \"Error: Invalid Input or API request failed.\"\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    data = {'Name': ['Alberta Hunter', 'Barrington Levy', 'Papa Charlie Jackson']}\n    df = pd.DataFrame(data)\n\n\n    df['Gender'] = df['Name'].apply(get_gender)\n    return df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/broken_river.py", "language": "python", "type": "data_loader", "uuid": "broken_river"}, "data_loaders/cool_fire.py:data_loader:python:cool fire": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\nimport pandas as pd\nfrom demo_project.custom.sparkconnection import spark\nfrom demo_project.custom.dataclass import datafiles\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.functions import when, rand\n# becuse i have a problem in resources i use this function to complete the project\ndef assign_gender(df):\n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"mostly_female\", \"female\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when((df[\"gender\"] == \"mostly_male\") | (df[\"gender\"] == \"andy\"), \"male\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"unknown\", \n                            when(rand() > 0.3, \"male\").otherwise(\"female\"))\n                       .otherwise(df[\"gender\"]))\n    \n    return df\n\n \ndef write_to_google_sheets(df, sheet_name, credentials_file,Name):\n    # Authenticate with Google Sheets using service account credentials\n    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n    credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_file, scope)\n    gc = gspread.authorize(credentials)\n    gender_count_df=df\n    print(gender_count_df)\n    header = gender_count_df.columns\n    data = [list(row) for row in gender_count_df.collect()]\n    print (data)\n    # Open the Google Sheets spreadsheet by name\n    spreadsheet = gc.open(sheet_name)\n    sheet = spreadsheet.worksheet(Name)\n    sheet.clear()  # Clear existing data\n    sheet.update([header]+data)  # Write header and data\n\n\n\n\n\n@transformer\ndef transform(*args, **kwargs):\n    artist_gender_df = spark.read.parquet(\"datafiles/unique_artists_with_gender\")\n    yearly_track_count_df = spark.read.parquet(\"datafiles/tracks_per_year\")\n\n    artist_gender_assigned_df = assign_gender(artist_gender_df)\n    gender_count_df = artist_gender_assigned_df.groupBy('gender').count().orderBy('count', ascending=True)\n    write_to_google_sheets(gender_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_count\")\n\n\n\n    gender_yearly_count_df = artist_gender_assigned_df.join(yearly_track_count_df, yearly_track_count_df.artistname == artist_gender_assigned_df.artistname).groupBy('gender','songyear').count()\n    write_to_google_sheets(gender_yearly_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_yearly\")\n\n    yearly_track_count_sorted_df = yearly_track_count_df.groupBy('songyear').count().orderBy('count', ascending=True)\n    write_to_google_sheets(yearly_track_count_sorted_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"yearly_track_count_sorted\")\n\n    \n    return yearly_track_count_sorted_df\n\n# pandas_df = pd.DataFrame({'Column1': [1, 23, 3], 'Column2': ['A', 'B', 'C']})\n# write_to_google_sheets(pandas_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\")\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/cool_fire.py", "language": "python", "type": "data_loader", "uuid": "cool_fire"}, "data_loaders/downladfiles.py:data_loader:python:downladfiles": {"content": "import urllib.request\nimport os\nfrom pyspark.sql import SparkSession\n\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nspark = SparkSession.builder.master(os.getenv('SPARK_MASTER_HOST', 'local')).getOrCreate()\n\ndef download_file(url, filename):\n    try:\n        urllib.request.urlretrieve(url, filename)\n        print(f\"{filename} downloaded successfully\")\n    except Exception as e:\n        print(f\"Error downloading {filename}: {e}\")\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    unique_artists = \"http://millionsongdataset.com/sites/default/files/AdditionalFiles/unique_artists.txt\"\n    tracks_per_year = \"http://millionsongdataset.com/sites/default/files/AdditionalFiles/tracks_per_year.txt\"\n    unique_tracks = \"http://millionsongdataset.com/sites/default/files/AdditionalFiles/unique_tracks.txt\"\n\n    urls=[unique_artists,tracks_per_year,unique_tracks]\n    for url in urls:\n        filename = os.path.basename(url)\n        if not os.path.exists(filename):\n            print(f\"{filename} downloading now\")\n            download_file(url, filename)\n        else :\n            print(f\"{filename} is already exist \")\n    return urls \n\n@test\ndef test_output(*args) -> None:\n    files=['unique_artists.txt','tracks_per_year.txt','unique_tracks.txt']\n    for filename in files :\n        assert os.path.exists(filename) ==True\n", "file_path": "data_loaders/downladfiles.py", "language": "python", "type": "data_loader", "uuid": "downladfiles"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/red_thunder.py:data_loader:python:red thunder": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom demo_project.custom.sparkconnection import spark\n\ndef get_gender(name):\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"format\": \"json\",\n        \"language\": \"en\",\n        \"search\": name\n    }\n    try:\n        data = requests.get(url, params=params)\n        gender_id = data.json()[\"search\"][0][\"id\"]\n        gender_url = f'https://www.wikidata.org/wiki/Special:EntityData/{gender_id}.json'\n        response = requests.get(gender_url)\n        gender_data = response.json()\n        try:\n            gender = gender_data['entities'][gender_id]['claims']['P21'][0]['mainsnak']['datavalue']['value']['id']\n            return gender\n        except KeyError:\n            return \"Error: Invalid JSON structure or key path not found.\"\n    except:\n        return \"Error: Invalid Input or API request failed.\"\n\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    # Get the genders\n\n    return unique_artists_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/red_thunder.py", "language": "python", "type": "data_loader", "uuid": "red_thunder"}, "data_loaders/weathered_butterfly.py:data_loader:python:weathered butterfly": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\nimport pandas as pd\nfrom demo_project.custom.sparkconnection import spark\nfrom demo_project.custom.dataclass import datafiles\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.functions import when, rand\n# becuse i have a problem in resources i use this function to complete the project\ndef assign_gender(df):\n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"mostly_female\", \"female\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when((df[\"gender\"] == \"mostly_male\") | (df[\"gender\"] == \"andy\"), \"male\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"unknown\", \n                            when(rand() > 0.3, \"male\").otherwise(\"female\"))\n                       .otherwise(df[\"gender\"]))\n    \n    return df\n\n \ndef write_to_google_sheets(df, sheet_name, credentials_file,Name):\n    # Authenticate with Google Sheets using service account credentials\n    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n    credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_file, scope)\n    gc = gspread.authorize(credentials)\n    gender_count_df=df\n    print(gender_count_df)\n    header = gender_count_df.columns\n    data = [list(row) for row in gender_count_df.collect()]\n    print (data)\n    # Open the Google Sheets spreadsheet by name\n    spreadsheet = gc.open(sheet_name)\n    sheet = spreadsheet.worksheet(Name)\n    sheet.clear()  # Clear existing data\n    sheet.update([header]+data)  # Write header and data\n\n\n\n\n\n@transformer\ndef transform(*args, **kwargs):\n    artist_gender_df = spark.read.parquet(\"datafiles/unique_artists_with_gender\")\n    yearly_track_count_df = spark.read.parquet(\"datafiles/tracks_per_year\")\n\n    artist_gender_assigned_df = assign_gender(artist_gender_df)\n    gender_count_df = artist_gender_assigned_df.groupBy('gender').count().orderBy('count', ascending=True)\n    write_to_google_sheets(gender_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_count\")\n\n\n\n    gender_yearly_count_df = artist_gender_assigned_df.join(yearly_track_count_df, yearly_track_count_df.artistname == artist_gender_assigned_df.artistname).groupBy('gender','songyear').count()\n    write_to_google_sheets(gender_yearly_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_yearly\")\n\n    yearly_track_count_sorted_df = yearly_track_count_df.groupBy('songyear').count().orderBy('count', ascending=True)\n    write_to_google_sheets(yearly_track_count_sorted_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"yearly_track_count_sorted\")\n\n    \n    return yearly_track_count_sorted_df\n\n# pandas_df = pd.DataFrame({'Column1': [1, 23, 3], 'Column2': ['A', 'B', 'C']})\n# write_to_google_sheets(pandas_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\")\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/weathered_butterfly.py", "language": "python", "type": "data_loader", "uuid": "weathered_butterfly"}, "data_loaders/white_river.py:data_loader:python:white river": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'your_bucket_name'\n    object_key = 'your_object_key'\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(\n        bucket_name,\n        object_key,\n    )\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/white_river.py", "language": "python", "type": "data_loader", "uuid": "white_river"}, "scratchpads/testtheapi.py:scratchpad:python:testtheapi": {"content": "import pandas as pd\nimport requests\nimport os\n\ndef get_gender(name):\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"format\": \"json\",\n        \"language\": \"en\",\n        \"search\": name\n    }\n    try:\n        data = requests.get(url, params=params)\n        gender_id = data.json()[\"search\"][0][\"id\"]\n        gender_url = f'https://www.wikidata.org/wiki/Special:EntityData/{gender_id}.json'\n        response = requests.get(gender_url)\n        gender_data = response.json()\n        try:\n            gender = gender_data['entities'][gender_id]['claims']['P21'][0]['mainsnak']['datavalue']['value']['id']\n            return gender\n        except KeyError:\n            return \"Error: Invalid JSON structure or key path not found.\"\n    except:\n        return \"Error: Invalid Input or API request failed.\"\n\ndata = {'Name': ['Alberta Hunter', 'Barrington Levy', 'Papa Charlie Jackson']}\ndf = pd.DataFrame(data)\n\n\ndf['Gender'] = df['Name'].apply(get_gender)\n\n\nprint(df)", "file_path": "scratchpads/testtheapi.py", "language": "python", "type": "scratchpad", "uuid": "testtheapi"}, "transformers/datawikiapi.py:transformer:python:datawikiapi": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nfrom demo_project.custom.sparkconnection import spark\nfrom pyspark.sql import types\nfrom pyspark.sql.functions import split\nfrom demo_project.custom.dataclass import datafiles\n\ndef get_gender(name):\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"format\": \"json\",\n        \"language\": \"en\",\n        \"search\": name\n    }\n    try:\n        data = requests.get(url, params=params)\n        gender_id = data.json()[\"search\"][0][\"id\"]\n        gender_url = f'https://www.wikidata.org/wiki/Special:EntityData/{gender_id}.json'\n        response = requests.get(gender_url)\n        gender_data = response.json()\n        try:\n            gender = gender_data['entities'][gender_id]['claims']['P21'][0]['mainsnak']['datavalue']['value']['id']\n            return gender\n        except KeyError:\n            return \"Error: Invalid JSON structure or key path not found.\"\n    except:\n        return \"Error: Invalid Input or API request failed.\"\n\n@transformer\ndef transform(*args, **kwargs):\n    unique_artists_df_par = spark.read.parquet(\"datafiles/unique_artists\")\n\n    artist_names = unique_artists_df_par.select('artistname').distinct().limit(10).rdd.flatMap(lambda x: x).collect()\n\n    genders = {name: get_gender(name) for name in artist_names}\n\n    genders_df = spark.createDataFrame(list(genders.items()), [\"artistname\", \"Gender\"])\n\n    result_df = unique_artists_df_par.join(genders_df, on='artistname', how='left')\n\n    result_df.show()\n\n\n    return result_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/datawikiapi.py", "language": "python", "type": "transformer", "uuid": "datawikiapi"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_age = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_age)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/handletextfiles.py:transformer:python:handletextfiles": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n    \nfrom demo_project.custom.sparkconnection import spark\nfrom pyspark.sql import types\nfrom pyspark.sql.functions import split\n\n\ndef process_file(file_name, columns, output_dir,numofpartitions):\n    df = spark.read.text(file_name).select(\n        split('value', '<SEP>').getItem(0).alias(columns[0]),\n        split('value', '<SEP>').getItem(1).alias(columns[1]),\n        split('value', '<SEP>').getItem(2).alias(columns[2]),\n        split('value', '<SEP>').getItem(3).alias(columns[3])\n    )\n    try:\n        df.repartition(numofpartitions).write.parquet(output_dir)\n    except:\n        print(f\"File already exists: {output_dir}\")\n\n    return df\n\n\n@transformer\ndef transform(*args, **kwargs):\n\n    unique_artists_df = process_file(\"unique_artists.txt\", ['artistid', 'artistmbid', 'trackid', 'artistname'], \"datafiles/unique_artists\",6)\n    tracks_per_year_df = process_file(\"tracks_per_year.txt\", ['songyear', 'uniqueID', 'artistname', 'Songname'], \"datafiles/tracks_per_year\",3)\n    unique_tracks_df = process_file(\"unique_tracks.txt\", ['trackid', 'songid', 'artistname', 'songtitle'], \"datafiles/unique_tracks\",3)\n\n    return unique_artists_df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/handletextfiles.py", "language": "python", "type": "transformer", "uuid": "handletextfiles"}, "transformers/lastformat_loadtogooglesheet.py:transformer:python:lastformat loadtogooglesheet": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\nimport pandas as pd\nfrom demo_project.custom.sparkconnection import spark\nfrom demo_project.custom.dataclass import datafiles\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.functions import when, rand\n# becuse i have a problem in resources i use this function to complete the project\ndef assign_gender(df):\n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"mostly_female\", \"female\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when((df[\"gender\"] == \"mostly_male\") | (df[\"gender\"] == \"andy\"), \"male\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"unknown\", \n                            when(rand() > 0.3, \"male\").otherwise(\"female\"))\n                       .otherwise(df[\"gender\"]))\n    \n    return df\n\n \ndef write_to_google_sheets(df, sheet_name, credentials_file,Name):\n    # Authenticate with Google Sheets using service account credentials\n    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n    credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_file, scope)\n    gc = gspread.authorize(credentials)\n    gender_count_df=df\n    print(gender_count_df)\n    header = gender_count_df.columns\n    data = [list(row) for row in gender_count_df.collect()]\n    print (data)\n    # Open the Google Sheets spreadsheet by name\n    spreadsheet = gc.open(sheet_name)\n    sheet = spreadsheet.worksheet(Name)\n    sheet.clear()  # Clear existing data\n    sheet.update([header]+data)  # Write header and data\n\n\n\n\n\n@transformer\ndef transform(*args, **kwargs):\n    artist_gender_df = spark.read.parquet(\"datafiles/unique_artists_with_gender\")\n    yearly_track_count_df = spark.read.parquet(\"datafiles/tracks_per_year\")\n\n    artist_gender_assigned_df = assign_gender(artist_gender_df)\n    gender_count_df = artist_gender_assigned_df.groupBy('gender').count().orderBy('count', ascending=True)\n    write_to_google_sheets(gender_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_count\")\n\n\n\n    gender_yearly_count_df = artist_gender_assigned_df.join(yearly_track_count_df, yearly_track_count_df.artistname == artist_gender_assigned_df.artistname).groupBy('gender','songyear').count()\n    write_to_google_sheets(gender_yearly_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_yearly\")\n\n    yearly_track_count_sorted_df = yearly_track_count_df.groupBy('songyear').count().orderBy('count', ascending=True)\n    write_to_google_sheets(yearly_track_count_sorted_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"yearly_track_count_sorted\")\n\n    \n    return yearly_track_count_sorted_df\n\n# pandas_df = pd.DataFrame({'Column1': [1, 23, 3], 'Column2': ['A', 'B', 'C']})\n# write_to_google_sheets(pandas_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\")\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/lastformat_loadtogooglesheet.py", "language": "python", "type": "transformer", "uuid": "lastformat_loadtogooglesheet"}, "transformers/lingering_haze.py:transformer:python:lingering haze": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom demo_project.custom.sparkconnection import spark\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import when, rand\nimport pandas as pd\nfrom google.cloud import bigquery\nfrom google_auth_oauthlib import flow\nfrom google.auth.transport.requests import Request\nfrom pandas_gbq import to_gbq\n\ndef assign_gender(df):\n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"mostly_female\", \"female\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when((df[\"gender\"] == \"mostly_male\") | (df[\"gender\"] == \"andy\"), \"male\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"unknown\", \n                            when(rand() > 0.3, \"male\").otherwise(\"female\"))\n                       .otherwise(df[\"gender\"]))\n    \n    return df\n\ndef save_to_bigquery(df, project_id, dataset_id, table_id):\n    # Create a BigQuery client\n    client = bigquery.Client(project=project_id)\n\n    # Convert DataFrame to Pandas DataFrame\n    pandas_df = df.toPandas()\n\n    # Upload to BigQuery\n    table_ref = client.dataset(dataset_id).table(table_id)\n    to_gbq(pandas_df, destination=table_ref, project_id=project_id, if_exists='replace')\n@transformer\ndef transform(*args, **kwargs):\n    artist_gender_df = spark.read.parquet(\"datafiles/unique_artists_with_gender\")\n    yearly_track_count_df = spark.read.parquet(\"datafiles/tracks_per_year\")\n\n    artist_gender_assigned_df = assign_gender(artist_gender_df)\n    \n    # Calculate gender count and save to BigQuery\n    gender_count_df = artist_gender_assigned_df.groupBy('gender').count().orderBy('count', ascending=True)\n    save_to_bigquery(gender_count_df, 'oval-bot-404221', 'your_dataset', 'gender_count')\n\n    # Calculate gender yearly count and save to BigQuery\n    gender_yearly_count_df = artist_gender_assigned_df.join(yearly_track_count_df, yearly_track_count_df.artistname == artist_gender_assigned_df.artistname).groupBy('gender','songyear').count()\n    save_to_bigquery(gender_yearly_count_df, 'oval-bot-404221', 'your_dataset', 'gender_yearly_count')\n\n    # Calculate yearly track count sorted and save to BigQuery\n    yearly_track_count_sorted_df = yearly_track_count_df.groupBy('songyear').count().orderBy('count', ascending=True)\n    save_to_bigquery(yearly_track_count_sorted_df, 'oval-bot-404221', 'your_dataset', 'yearly_track_count_sorted')\n    \n    return yearly_track_count_sorted_df\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/lingering_haze.py", "language": "python", "type": "transformer", "uuid": "lingering_haze"}, "transformers/loadtogooglesheet.py:transformer:python:loadtogooglesheet": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\nimport pandas as pd\nfrom demo_project.custom.sparkconnection import spark\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.functions import when, rand\n# becuse i have a problem in resources i use this function to complete the project if you know a methode please help\ndef assign_gender(df):\n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"mostly_female\", \"female\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when((df[\"gender\"] == \"mostly_male\") | (df[\"gender\"] == \"andy\"), \"male\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"unknown\", \n                            when(rand() > 0.3, \"male\").otherwise(\"female\"))\n                       .otherwise(df[\"gender\"]))\n    \n    return df\n\n \ndef write_to_google_sheets(df, sheet_name, credentials_file,Name):\n    # Authenticate with Google Sheets using service account credentials\n    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n    credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_file, scope)\n    gc = gspread.authorize(credentials)\n    gender_count_df=df\n    print(gender_count_df)\n    header = gender_count_df.columns\n    data = [list(row) for row in gender_count_df.collect()]\n    print (data)\n    # Open the Google Sheets spreadsheet by name\n    spreadsheet = gc.open(sheet_name)\n    sheet = spreadsheet.worksheet(Name)\n    sheet.clear()  # Clear existing data\n    sheet.update([header]+data)  # Write header and data\n\n\n\n\n\n@transformer\ndef transform(*args, **kwargs):\n    artist_gender_df = spark.read.parquet(\"datafiles/unique_artists_with_gender\")\n    yearly_track_count_df = spark.read.parquet(\"datafiles/tracks_per_year\")\n\n    artist_gender_assigned_df = assign_gender(artist_gender_df)\n    gender_count_df = artist_gender_assigned_df.groupBy('gender').count().orderBy('count', ascending=True)\n    write_to_google_sheets(gender_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_count\")\n\n\n\n    gender_yearly_count_df = artist_gender_assigned_df.join(yearly_track_count_df, yearly_track_count_df.artistname == artist_gender_assigned_df.artistname).groupBy('gender','songyear').count()\n    write_to_google_sheets(gender_yearly_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_yearly\")\n\n    yearly_track_count_sorted_df = yearly_track_count_df.groupBy('songyear').count().orderBy('count', ascending=True)\n    write_to_google_sheets(yearly_track_count_sorted_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"yearly_track_count_sorted\")\n\n    \n    return yearly_track_count_sorted_df\n\n# pandas_df = pd.DataFrame({'Column1': [1, 23, 3], 'Column2': ['A', 'B', 'C']})\n# write_to_google_sheets(pandas_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\")\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/loadtogooglesheet.py", "language": "python", "type": "transformer", "uuid": "loadtogooglesheet"}, "transformers/snowy_grass.py:transformer:python:snowy grass": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport requests\nfrom pyspark.sql import SparkSession, functions as F\nimport gender_guesser.detector as gender\nfrom pyspark.sql.functions import udf\n\nimport os\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom demo_project.custom.sparkconnection import spark\nimport time\n\ndef get_gender(name):\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"format\": \"json\",\n        \"language\": \"en\",\n        \"search\": name}\n    try:\n        data = requests.get(url, params=params)\n\n        wikidataID = data.json()[\"search\"][0][\"id\"]\n\n        gender_url = f'https://www.wikidata.org/wiki/Special:EntityData/{wikidataID}.json'\n        response = requests.get(gender_url)\n        gender_data = response.json()\n        try:\n            gender = gender_data['entities'][wikidataID]['claims']['P21'][0]['mainsnak']['datavalue']['value']['id']\n            return gender\n        except KeyError:\n            return \"Error: Invalid JSON structure or key path not found.\"\n    except:\n        return \"null\"\n\n@transformer\ndef transform(*args, **kwargs):\n    df=spark.read.parquet(\"datafiles/unique_artists\")\n    if os.path.exists('datafiles/unique_artists_with_gender'):\n        pass \n\n    else:\n        udf_gender = F.udf(get_gender_ai, F.StringType())  # Create reusable UDF\n\n        transformed_df = df.withColumn(\"gender\", udf_gender(F.col(\"artistname\")))\n        transformed_df.repartition(6).write.parquet(\"datafiles/unique_artists_with_gender\")\n    #  i used AI to save time and resources i tried to use the api but every request take 1 sec and i did't find a way to optimize that ,help is welocme\n\n    return \"done\"\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/snowy_grass.py", "language": "python", "type": "transformer", "uuid": "snowy_grass"}, "/home/src/demo_project/custom/sparkconnection.py:custom:python:home/src/demo project/custom/sparkconnection": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport os\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.master(os.getenv('SPARK_MASTER_HOST', 'local')).getOrCreate()\n\n@custom\ndef transform_custom(*args, **kwargs):\n\n    return spark.sql('select 1')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/demo_project/custom/sparkconnection.py", "language": "python", "type": "custom", "uuid": "sparkconnection"}, "/home/src/demo_project/data_loaders/downladfiles.py:data_loader:python:home/src/demo project/data loaders/downladfiles": {"content": "import urllib.request\nimport os\nfrom pyspark.sql import SparkSession\n\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nspark = SparkSession.builder.master(os.getenv('SPARK_MASTER_HOST', 'local')).getOrCreate()\n\ndef download_file(url, filename):\n    try:\n        urllib.request.urlretrieve(url, filename)\n        print(f\"{filename} downloaded successfully\")\n    except Exception as e:\n        print(f\"Error downloading {filename}: {e}\")\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n\n    unique_artists = \"http://millionsongdataset.com/sites/default/files/AdditionalFiles/unique_artists.txt\"\n    tracks_per_year = \"http://millionsongdataset.com/sites/default/files/AdditionalFiles/tracks_per_year.txt\"\n    unique_tracks = \"http://millionsongdataset.com/sites/default/files/AdditionalFiles/unique_tracks.txt\"\n\n    urls=[unique_artists,tracks_per_year,unique_tracks]\n    for url in urls:\n        filename = os.path.basename(url)\n        if not os.path.exists(filename):\n            print(f\"{filename} downloading now\")\n            download_file(url, filename)\n        else :\n            print(f\"{filename} is already exist \")\n    return urls \n\n@test\ndef test_output(*args) -> None:\n    files=['unique_artists.txt','tracks_per_year.txt','unique_tracks.txt']\n    for filename in files :\n        assert os.path.exists(filename) ==True\n", "file_path": "/home/src/demo_project/data_loaders/downladfiles.py", "language": "python", "type": "data_loader", "uuid": "downladfiles"}, "/home/src/demo_project/transformers/handletextfiles.py:transformer:python:home/src/demo project/transformers/handletextfiles": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n    \nfrom demo_project.custom.sparkconnection import spark\nfrom pyspark.sql import types\nfrom pyspark.sql.functions import split\n\n\ndef process_file(file_name, columns, output_dir,numofpartitions):\n    df = spark.read.text(file_name).select(\n        split('value', '<SEP>').getItem(0).alias(columns[0]),\n        split('value', '<SEP>').getItem(1).alias(columns[1]),\n        split('value', '<SEP>').getItem(2).alias(columns[2]),\n        split('value', '<SEP>').getItem(3).alias(columns[3])\n    )\n    try:\n        df.repartition(numofpartitions).write.parquet(output_dir)\n    except:\n        print(f\"File already exists: {output_dir}\")\n\n    return df\n\n\n@transformer\ndef transform(*args, **kwargs):\n\n    unique_artists_df = process_file(\"unique_artists.txt\", ['artistid', 'artistmbid', 'trackid', 'artistname'], \"datafiles/unique_artists\",6)\n    tracks_per_year_df = process_file(\"tracks_per_year.txt\", ['songyear', 'uniqueID', 'artistname', 'Songname'], \"datafiles/tracks_per_year\",3)\n    unique_tracks_df = process_file(\"unique_tracks.txt\", ['trackid', 'songid', 'artistname', 'songtitle'], \"datafiles/unique_tracks\",3)\n\n    return unique_artists_df\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/demo_project/transformers/handletextfiles.py", "language": "python", "type": "transformer", "uuid": "handletextfiles"}, "/home/src/demo_project/scratchpads/testtheapi.py:scratchpad:python:home/src/demo project/scratchpads/testtheapi": {"content": "import pandas as pd\nimport requests\nimport os\n\ndef get_gender(name):\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"format\": \"json\",\n        \"language\": \"en\",\n        \"search\": name\n    }\n    try:\n        data = requests.get(url, params=params)\n        gender_id = data.json()[\"search\"][0][\"id\"]\n        gender_url = f'https://www.wikidata.org/wiki/Special:EntityData/{gender_id}.json'\n        response = requests.get(gender_url)\n        gender_data = response.json()\n        try:\n            gender = gender_data['entities'][gender_id]['claims']['P21'][0]['mainsnak']['datavalue']['value']['id']\n            return gender\n        except KeyError:\n            return \"Error: Invalid JSON structure or key path not found.\"\n    except:\n        return \"Error: Invalid Input or API request failed.\"\n\ndata = {'Name': ['Alberta Hunter', 'Barrington Levy', 'Papa Charlie Jackson']}\ndf = pd.DataFrame(data)\n\n\ndf['Gender'] = df['Name'].apply(get_gender)\n\n\nprint(df)", "file_path": "/home/src/demo_project/scratchpads/testtheapi.py", "language": "python", "type": "scratchpad", "uuid": "testtheapi"}, "/home/src/demo_project/transformers/snowy_grass.py:transformer:python:home/src/demo project/transformers/snowy grass": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport requests\nfrom pyspark.sql import SparkSession, functions as F\nimport gender_guesser.detector as gender\nfrom pyspark.sql.functions import udf\n\nimport os\n\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nfrom demo_project.custom.sparkconnection import spark\nimport time\n\ndef get_gender(name):\n    url = \"https://www.wikidata.org/w/api.php\"\n    params = {\n        \"action\": \"wbsearchentities\",\n        \"format\": \"json\",\n        \"language\": \"en\",\n        \"search\": name}\n    try:\n        data = requests.get(url, params=params)\n\n        wikidataID = data.json()[\"search\"][0][\"id\"]\n\n        gender_url = f'https://www.wikidata.org/wiki/Special:EntityData/{wikidataID}.json'\n        response = requests.get(gender_url)\n        gender_data = response.json()\n        try:\n            gender = gender_data['entities'][wikidataID]['claims']['P21'][0]['mainsnak']['datavalue']['value']['id']\n            return gender\n        except KeyError:\n            return \"Error: Invalid JSON structure or key path not found.\"\n    except:\n        return \"null\"\n\n@transformer\ndef transform(*args, **kwargs):\n    df=spark.read.parquet(\"datafiles/unique_artists\")\n    if os.path.exists('datafiles/unique_artists_with_gender'):\n        pass \n\n    else:\n        udf_gender = F.udf(get_gender_ai, F.StringType())  # Create reusable UDF\n\n        transformed_df = df.withColumn(\"gender\", udf_gender(F.col(\"artistname\")))\n        transformed_df.repartition(6).write.parquet(\"datafiles/unique_artists_with_gender\")\n    #  i used AI to save time and resources i tried to use the api but every request take 1 sec and i did't find a way to optimize that ,help is welocme\n\n    return \"done\"\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/demo_project/transformers/snowy_grass.py", "language": "python", "type": "transformer", "uuid": "snowy_grass"}, "/home/src/demo_project/transformers/loadtogooglesheet.py:transformer:python:home/src/demo project/transformers/loadtogooglesheet": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom oauth2client.service_account import ServiceAccountCredentials\nimport gspread\nimport pandas as pd\nfrom demo_project.custom.sparkconnection import spark\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.functions import when, rand\n# becuse i have a problem in resources i use this function to complete the project if you know a methode please help\ndef assign_gender(df):\n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"mostly_female\", \"female\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when((df[\"gender\"] == \"mostly_male\") | (df[\"gender\"] == \"andy\"), \"male\")\n                       .otherwise(df[\"gender\"]))\n    \n    df = df.withColumn(\"gender\", \n                       when(df[\"gender\"] == \"unknown\", \n                            when(rand() > 0.3, \"male\").otherwise(\"female\"))\n                       .otherwise(df[\"gender\"]))\n    \n    return df\n\n \ndef write_to_google_sheets(df, sheet_name, credentials_file,Name):\n    # Authenticate with Google Sheets using service account credentials\n    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n    credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_file, scope)\n    gc = gspread.authorize(credentials)\n    gender_count_df=df\n    print(gender_count_df)\n    header = gender_count_df.columns\n    data = [list(row) for row in gender_count_df.collect()]\n    print (data)\n    # Open the Google Sheets spreadsheet by name\n    spreadsheet = gc.open(sheet_name)\n    sheet = spreadsheet.worksheet(Name)\n    sheet.clear()  # Clear existing data\n    sheet.update([header]+data)  # Write header and data\n\n\n\n\n\n@transformer\ndef transform(*args, **kwargs):\n    artist_gender_df = spark.read.parquet(\"datafiles/unique_artists_with_gender\")\n    yearly_track_count_df = spark.read.parquet(\"datafiles/tracks_per_year\")\n\n    artist_gender_assigned_df = assign_gender(artist_gender_df)\n    gender_count_df = artist_gender_assigned_df.groupBy('gender').count().orderBy('count', ascending=True)\n    write_to_google_sheets(gender_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_count\")\n\n\n\n    gender_yearly_count_df = artist_gender_assigned_df.join(yearly_track_count_df, yearly_track_count_df.artistname == artist_gender_assigned_df.artistname).groupBy('gender','songyear').count()\n    write_to_google_sheets(gender_yearly_count_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"gender_yearly\")\n\n    yearly_track_count_sorted_df = yearly_track_count_df.groupBy('songyear').count().orderBy('count', ascending=True)\n    write_to_google_sheets(yearly_track_count_sorted_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\",\"yearly_track_count_sorted\")\n\n    \n    return yearly_track_count_sorted_df\n\n# pandas_df = pd.DataFrame({'Column1': [1, 23, 3], 'Column2': ['A', 'B', 'C']})\n# write_to_google_sheets(pandas_df, \"hi\", \"ancient-bond-413701-80a01d0a4a8b.json\")\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/demo_project/transformers/loadtogooglesheet.py", "language": "python", "type": "transformer", "uuid": "loadtogooglesheet"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}